{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tomomi_Gestures_YOLOv3.ipynb","provenance":[],"collapsed_sections":["Uy30FG-sWwEe","0K0Crr7xXzMc","X5dME2neW_cN","Pcd7yGyAeWHk","2lTE3CVoepqo"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TKk9fJiXWN20","executionInfo":{"status":"ok","timestamp":1619297312501,"user_tz":240,"elapsed":23875,"user":{"displayName":"Tomomi Bahun","photoUrl":"","userId":"12246148042428186562"}},"outputId":"ec3e6d6f-43a4-4539-adb6-ab2619fc66e4"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wSEjMzFReHIh"},"source":["Majority of code borrowed from https://github.com/cfotache/pytorch_custom_yolo_training."]},{"cell_type":"markdown","metadata":{"id":"Uy30FG-sWwEe"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"KKGwmpYUWWh-","executionInfo":{"status":"ok","timestamp":1619297316823,"user_tz":240,"elapsed":28175,"user":{"displayName":"Tomomi Bahun","photoUrl":"","userId":"12246148042428186562"}}},"source":["import sys\n","import glob\n","import random\n","import os\n","import torchvision.transforms as transforms\n","from skimage.transform import resize\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import numpy as np\n","\n","from torch.utils.data import Dataset\n","from PIL import Image\n","from collections import defaultdict\n","  \n","from __future__ import division\n","import math\n","import time\n","import datetime\n","import argparse\n","\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision import transforms\n","import torch.optim as optim\n","import torchvision\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from matplotlib.ticker import NullLocator"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0K0Crr7xXzMc"},"source":["## Utilities"]},{"cell_type":"code","metadata":{"id":"SUb83amSX0z0","executionInfo":{"status":"ok","timestamp":1619297317003,"user_tz":240,"elapsed":28338,"user":{"displayName":"Tomomi Bahun","photoUrl":"","userId":"12246148042428186562"}}},"source":["def load_classes(path):\n","    \"\"\"\n","    Loads class labels at 'path'\n","    \"\"\"\n","    fp = open(path, \"r\")\n","    names = fp.read().split(\"\\n\")[:-1]\n","    return names\n","\n","\n","def weights_init_normal(m):\n","    classname = m.__class__.__name__\n","    if classname.find(\"Conv\") != -1:\n","        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n","    elif classname.find(\"BatchNorm2d\") != -1:\n","        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n","        torch.nn.init.constant_(m.bias.data, 0.0)\n","\n","\n","def compute_ap(recall, precision):\n","    \"\"\" Compute the average precision, given the recall and precision curves.\n","    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n","    # Arguments\n","        recall:    The recall curve (list).\n","        precision: The precision curve (list).\n","    # Returns\n","        The average precision as computed in py-faster-rcnn.\n","    \"\"\"\n","    # correct AP calculation\n","    # first append sentinel values at the end\n","    mrec = np.concatenate(([0.0], recall, [1.0]))\n","    mpre = np.concatenate(([0.0], precision, [0.0]))\n","\n","    # compute the precision envelope\n","    for i in range(mpre.size - 1, 0, -1):\n","        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n","\n","    # to calculate area under PR curve, look for points\n","    # where X axis (recall) changes value\n","    i = np.where(mrec[1:] != mrec[:-1])[0]\n","\n","    # and sum (\\Delta recall) * prec\n","    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n","    return ap\n","\n","\n","def bbox_iou(box1, box2, x1y1x2y2=True):\n","    \"\"\"\n","    Returns the IoU of two bounding boxes\n","    \"\"\"\n","    if not x1y1x2y2:\n","        # Transform from center and width to exact coordinates\n","        b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2\n","        b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2\n","        b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2\n","        b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2\n","    else:\n","        # Get the coordinates of bounding boxes\n","        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\n","        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\n","\n","    # get the corrdinates of the intersection rectangle\n","    inter_rect_x1 = torch.max(b1_x1, b2_x1)\n","    inter_rect_y1 = torch.max(b1_y1, b2_y1)\n","    inter_rect_x2 = torch.min(b1_x2, b2_x2)\n","    inter_rect_y2 = torch.min(b1_y2, b2_y2)\n","    # Intersection area\n","    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * torch.clamp(\n","        inter_rect_y2 - inter_rect_y1 + 1, min=0\n","    )\n","    # Union Area\n","    b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n","    b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n","\n","    iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n","\n","    return iou\n","\n","\n","def bbox_iou_numpy(box1, box2):\n","    \"\"\"Computes IoU between bounding boxes.\n","    Parameters\n","    ----------\n","    box1 : ndarray\n","        (N, 4) shaped array with bboxes\n","    box2 : ndarray\n","        (M, 4) shaped array with bboxes\n","    Returns\n","    -------\n","    : ndarray\n","        (N, M) shaped array with IoUs\n","    \"\"\"\n","    area = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])\n","\n","    iw = np.minimum(np.expand_dims(box1[:, 2], axis=1), box2[:, 2]) - np.maximum(\n","        np.expand_dims(box1[:, 0], 1), box2[:, 0]\n","    )\n","    ih = np.minimum(np.expand_dims(box1[:, 3], axis=1), box2[:, 3]) - np.maximum(\n","        np.expand_dims(box1[:, 1], 1), box2[:, 1]\n","    )\n","\n","    iw = np.maximum(iw, 0)\n","    ih = np.maximum(ih, 0)\n","\n","    ua = np.expand_dims((box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1]), axis=1) + area - iw * ih\n","\n","    ua = np.maximum(ua, np.finfo(float).eps)\n","\n","    intersection = iw * ih\n","\n","    return intersection / ua\n","\n","\n","def build_targets(\n","    pred_boxes, pred_conf, pred_cls, target, anchors, num_anchors, num_classes, grid_size, ignore_thres, img_dim\n","):\n","    nB = target.size(0)\n","    nA = num_anchors\n","    nC = num_classes\n","    nG = grid_size\n","    mask = torch.zeros(nB, nA, nG, nG)\n","    conf_mask = torch.ones(nB, nA, nG, nG)\n","    tx = torch.zeros(nB, nA, nG, nG)\n","    ty = torch.zeros(nB, nA, nG, nG)\n","    tw = torch.zeros(nB, nA, nG, nG)\n","    th = torch.zeros(nB, nA, nG, nG)\n","    tconf = torch.ByteTensor(nB, nA, nG, nG).fill_(0)\n","    tcls = torch.ByteTensor(nB, nA, nG, nG, nC).fill_(0)\n","\n","    nGT = 0\n","    nCorrect = 0\n","    for b in range(nB):\n","        for t in range(target.shape[1]):\n","            if target[b, t].sum() == 0:\n","                continue\n","            nGT += 1\n","            # Convert to position relative to box\n","            gx = target[b, t, 1] * nG\n","            gy = target[b, t, 2] * nG\n","            gw = target[b, t, 3] * nG\n","            gh = target[b, t, 4] * nG\n","            # Get grid box indices\n","            gi = int(gx)\n","            gj = int(gy)\n","            # Get shape of gt box\n","            gt_box = torch.FloatTensor(np.array([0, 0, gw, gh])).unsqueeze(0)\n","            # Get shape of anchor box\n","            anchor_shapes = torch.FloatTensor(np.concatenate((np.zeros((len(anchors), 2)), np.array(anchors)), 1))\n","            # Calculate iou between gt and anchor shapes\n","            anch_ious = bbox_iou(gt_box, anchor_shapes)\n","            # Where the overlap is larger than threshold set mask to zero (ignore)\n","            conf_mask[b, anch_ious > ignore_thres, gj, gi] = 0\n","            # Find the best matching anchor box\n","            best_n = np.argmax(anch_ious)\n","            # Get ground truth box\n","            gt_box = torch.FloatTensor(np.array([gx, gy, gw, gh])).unsqueeze(0)\n","            # Get the best prediction\n","            pred_box = pred_boxes[b, best_n, gj, gi].unsqueeze(0)\n","            # Masks\n","            mask[b, best_n, gj, gi] = 1\n","            conf_mask[b, best_n, gj, gi] = 1\n","            # Coordinates\n","            tx[b, best_n, gj, gi] = gx - gi\n","            ty[b, best_n, gj, gi] = gy - gj\n","            # Width and height\n","            tw[b, best_n, gj, gi] = math.log(gw / anchors[best_n][0] + 1e-16)\n","            th[b, best_n, gj, gi] = math.log(gh / anchors[best_n][1] + 1e-16)\n","            # One-hot encoding of label\n","            target_label = int(target[b, t, 0])\n","            tcls[b, best_n, gj, gi, target_label] = 1\n","            tconf[b, best_n, gj, gi] = 1\n","\n","            # Calculate iou between ground truth and best matching prediction\n","            iou = bbox_iou(gt_box, pred_box, x1y1x2y2=False)\n","            pred_label = torch.argmax(pred_cls[b, best_n, gj, gi])\n","            score = pred_conf[b, best_n, gj, gi]\n","            if iou > 0.5 and pred_label == target_label and score > 0.5:\n","                nCorrect += 1\n","\n","    return nGT, nCorrect, mask, conf_mask, tx, ty, tw, th, tconf, tcls\n","\n","\n","def to_categorical(y, num_classes):\n","    \"\"\" 1-hot encodes a tensor \"\"\"\n","    return torch.from_numpy(np.eye(num_classes, dtype=\"uint8\")[y])"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OhghAv4xP1Az"},"source":["Below code borrowed from: https://github.com/eriklindernoren/PyTorch-YOLOv3/blob/master/pytorchyolo/utils/utils.py"]},{"cell_type":"code","metadata":{"id":"t8aXxW3GP1Z-","executionInfo":{"status":"ok","timestamp":1619297317005,"user_tz":240,"elapsed":28321,"user":{"displayName":"Tomomi Bahun","photoUrl":"","userId":"12246148042428186562"}}},"source":["def non_max_suppression(prediction, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False, labels=()):\n","    \"\"\"Performs Non-Maximum Suppression (NMS) on inference results\n","    Returns:\n","         detections with shape: nx6 (x1, y1, x2, y2, conf, cls)\n","    \"\"\"\n","\n","    nc = prediction.shape[2] - 5  # number of classes\n","    xc = prediction[..., 4] > conf_thres  # candidates\n","\n","    # Settings\n","    # (pixels) minimum and maximum box width and height\n","    max_wh = 4096\n","    max_det = 300  # maximum number of detections per image\n","    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n","    time_limit = 1.0  # seconds to quit after\n","    redundant = True  # require redundant detections\n","    multi_label = nc > 1  # multiple labels per box (adds 0.5ms/img)\n","    merge = True  # use merge-NMS\n","\n","    t = time.time()\n","    output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n","\n","    for xi, x in enumerate(prediction):  # image index, image inference\n","        # Apply constraints\n","        # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n","        x = x[xc[xi]]  # confidence\n","\n","        # Cat apriori labels if autolabelling\n","        if labels and len(labels[xi]):\n","            l = labels[xi]\n","            v = torch.zeros((len(l), nc + 5), device=x.device)\n","            v[:, :4] = l[:, 1:5]  # box\n","            v[:, 4] = 1.0  # conf\n","            v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n","            x = torch.cat((x, v), 0)\n","\n","        # If none remain process next image\n","        if not x.shape[0]:\n","            continue\n","\n","        # Compute conf\n","        x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n","\n","        # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n","        box = xywh2xyxy(x[:, :4])\n","\n","        # Detections matrix nx6 (xyxy, conf, cls)\n","        if multi_label:\n","            i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n","            x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n","        else:  # best class only\n","            conf, j = x[:, 5:].max(1, keepdim=True)\n","            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n","\n","        # Filter by class\n","        if classes is not None:\n","            x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n","\n","        # Apply finite constraint\n","        # if not torch.isfinite(x).all():\n","        #     x = x[torch.isfinite(x).all(1)]\n","\n","        # Check shape\n","        n = x.shape[0]  # number of boxes\n","        if not n:  # no boxes\n","            continue\n","        elif n > max_nms:  # excess boxes\n","            # sort by confidence\n","            x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n","\n","        # Batched NMS\n","        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n","        # boxes (offset by class), scores\n","        boxes, scores = x[:, :4] + c, x[:, 4]\n","        i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n","        if i.shape[0] > max_det:  # limit detections\n","            i = i[:max_det]\n","        if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n","            # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n","            iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n","            weights = iou * scores[None]  # box weights\n","            x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n","            if redundant:\n","                i = i[iou.sum(1) > 1]  # require redundancy\n","\n","        output[xi] = x[i]\n","        if (time.time() - t) > time_limit:\n","            print(f'WARNING: NMS time limit {time_limit}s exceeded')\n","            break  # time limit exceeded\n","\n","    return output\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X5dME2neW_cN"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"id":"GEwsgE51W-e4","executionInfo":{"status":"ok","timestamp":1619297317154,"user_tz":240,"elapsed":28458,"user":{"displayName":"Tomomi Bahun","photoUrl":"","userId":"12246148042428186562"}}},"source":["class ImageFolder(Dataset):\n","    def __init__(self, folder_path, img_size=416):\n","        self.files = sorted(glob.glob('%s/*.*' % folder_path))\n","        self.img_shape = (img_size, img_size)\n","\n","    def __getitem__(self, index):\n","        img_path = self.files[index % len(self.files)]\n","        # Extract image\n","        img = np.array(Image.open(img_path))\n","        h, w, _ = img.shape\n","        dim_diff = np.abs(h - w)\n","        # Upper (left) and lower (right) padding\n","        pad1, pad2 = dim_diff // 2, dim_diff - dim_diff // 2\n","        # Determine padding\n","        pad = ((pad1, pad2), (0, 0), (0, 0)) if h <= w else ((0, 0), (pad1, pad2), (0, 0))\n","        # Add padding\n","        input_img = np.pad(img, pad, 'constant', constant_values=127.5) / 255.\n","        # Resize and normalize\n","        input_img = resize(input_img, (*self.img_shape, 3), mode='reflect')\n","        # Channels-first\n","        input_img = np.transpose(input_img, (2, 0, 1))\n","        # As pytorch tensor\n","        input_img = torch.from_numpy(input_img).float()\n","\n","        return img_path, input_img\n","\n","    def __len__(self):\n","        return len(self.files)\n","\n","\n","class ListDataset(Dataset):\n","    def __init__(self, list_path, img_size=416):\n","        with open(list_path, 'r') as file:\n","            self.img_files = file.readlines()\n","        self.label_files = [path.replace('images', 'labels').replace('.jpg', '.txt') for path in self.img_files]\n","        self.img_shape = (img_size, img_size)\n","        self.max_objects = 1\n","\n","    def __getitem__(self, index):\n","\n","        #---------\n","        #  Image\n","        #---------\n","\n","        img_path = self.img_files[index % len(self.img_files)].rstrip()\n","        img = np.array(Image.open(img_path))\n","\n","        # Handles images with less than three channels\n","        while len(img.shape) != 3:\n","            index += 1\n","            img_path = self.img_files[index % len(self.img_files)].rstrip()\n","            img = np.array(Image.open(img_path))\n","\n","        h, w, _ = img.shape\n","        dim_diff = np.abs(h - w)\n","        # Upper (left) and lower (right) padding\n","        pad1, pad2 = dim_diff // 2, dim_diff - dim_diff // 2\n","        # Determine padding\n","        pad = ((pad1, pad2), (0, 0), (0, 0)) if h <= w else ((0, 0), (pad1, pad2), (0, 0))\n","        # Add padding\n","        input_img = np.pad(img, pad, 'constant', constant_values=128) / 255.\n","        padded_h, padded_w, _ = input_img.shape\n","        # Resize and normalize\n","        input_img = resize(input_img, (*self.img_shape, 3), mode='reflect')\n","        # Channels-first\n","        input_img = np.transpose(input_img, (2, 0, 1))\n","        # As pytorch tensor\n","        input_img = torch.from_numpy(input_img).float()\n","\n","        #---------\n","        #  Label\n","        #---------\n","\n","        label_path = self.label_files[index % len(self.img_files)].rstrip()\n","\n","        labels = None\n","        if os.path.exists(label_path):\n","            labels = np.loadtxt(label_path).reshape(-1, 5)\n","            # Extract coordinates for unpadded + unscaled image\n","            x1 = w * (labels[:, 1] - labels[:, 3]/2)\n","            y1 = h * (labels[:, 2] - labels[:, 4]/2)\n","            x2 = w * (labels[:, 1] + labels[:, 3]/2)\n","            y2 = h * (labels[:, 2] + labels[:, 4]/2)\n","            # Adjust for added padding\n","            x1 += pad[1][0]\n","            y1 += pad[0][0]\n","            x2 += pad[1][0]\n","            y2 += pad[0][0]\n","            # Calculate ratios from coordinates\n","            labels[:, 1] = ((x1 + x2) / 2) / padded_w\n","            labels[:, 2] = ((y1 + y2) / 2) / padded_h\n","            labels[:, 3] *= w / padded_w\n","            labels[:, 4] *= h / padded_h\n","        # Fill matrix\n","        filled_labels = np.zeros((self.max_objects, 5))\n","        if labels is not None:\n","            filled_labels[range(len(labels))[:self.max_objects]] = labels[:self.max_objects]\n","        filled_labels = torch.from_numpy(filled_labels)\n","\n","        return img_path, input_img, filled_labels\n","\n","    def __len__(self):\n","        return len(self.img_files)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rT_uV680XNJg"},"source":["## Model Creation"]},{"cell_type":"markdown","metadata":{"id":"Pcd7yGyAeWHk"},"source":["### Parsing of Configs"]},{"cell_type":"code","metadata":{"id":"2UQId47RXPc1","executionInfo":{"status":"ok","timestamp":1619297317156,"user_tz":240,"elapsed":28453,"user":{"displayName":"Tomomi Bahun","photoUrl":"","userId":"12246148042428186562"}}},"source":["def parse_model_config(path):\n","    \"\"\"Parses the yolo-v3 layer configuration file and returns module definitions\"\"\"\n","    file = open(path, 'r')\n","    lines = file.read().split('\\n')\n","    lines = [x for x in lines if x and not x.startswith('#')]\n","    lines = [x.rstrip().lstrip() for x in lines] # get rid of fringe whitespaces\n","    module_defs = []\n","    for line in lines:\n","        if line.startswith('['): # This marks the start of a new block\n","            module_defs.append({})\n","            module_defs[-1]['type'] = line[1:-1].rstrip()\n","            if module_defs[-1]['type'] == 'convolutional':\n","                module_defs[-1]['batch_normalize'] = 0\n","        else:\n","            key, value = line.split(\"=\")\n","            value = value.strip()\n","            module_defs[-1][key.rstrip()] = value.strip()\n","\n","    return module_defs\n","\n","def parse_data_config(path):\n","    \"\"\"Parses the data configuration file\"\"\"\n","    options = dict()\n","    options['gpus'] = '0,1,2,3'\n","    options['num_workers'] = '10'\n","    with open(path, 'r') as fp:\n","        lines = fp.readlines()\n","    for line in lines:\n","        line = line.strip()\n","        if line == '' or line.startswith('#'):\n","            continue\n","        key, value = line.split('=')\n","        options[key.strip()] = value.strip()\n","    return options"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2lTE3CVoepqo"},"source":["### Model Building"]},{"cell_type":"code","metadata":{"id":"JTYFp7MNXRY-","executionInfo":{"status":"ok","timestamp":1619297318428,"user_tz":240,"elapsed":29719,"user":{"displayName":"Tomomi Bahun","photoUrl":"","userId":"12246148042428186562"}}},"source":["def create_modules(module_defs):\n","    \"\"\"\n","    Constructs module list of layer blocks from module configuration in module_defs\n","    \"\"\"\n","    hyperparams = module_defs.pop(0)\n","    output_filters = [int(hyperparams[\"channels\"])]\n","    module_list = nn.ModuleList()\n","    for i, module_def in enumerate(module_defs):\n","        modules = nn.Sequential()\n","\n","        if module_def[\"type\"] == \"convolutional\":\n","            bn = int(module_def[\"batch_normalize\"])\n","            filters = int(module_def[\"filters\"])\n","            kernel_size = int(module_def[\"size\"])\n","            pad = (kernel_size - 1) // 2 if int(module_def[\"pad\"]) else 0\n","            modules.add_module(\n","                \"conv_%d\" % i,\n","                nn.Conv2d(\n","                    in_channels=output_filters[-1],\n","                    out_channels=filters,\n","                    kernel_size=kernel_size,\n","                    stride=int(module_def[\"stride\"]),\n","                    padding=pad,\n","                    bias=not bn,\n","                ),\n","            )\n","            if bn:\n","                modules.add_module(\"batch_norm_%d\" % i, nn.BatchNorm2d(filters))\n","            if module_def[\"activation\"] == \"leaky\":\n","                modules.add_module(\"leaky_%d\" % i, nn.LeakyReLU(0.1))\n","\n","        elif module_def[\"type\"] == \"maxpool\":\n","            kernel_size = int(module_def[\"size\"])\n","            stride = int(module_def[\"stride\"])\n","            if kernel_size == 2 and stride == 1:\n","                padding = nn.ZeroPad2d((0, 1, 0, 1))\n","                modules.add_module(\"_debug_padding_%d\" % i, padding)\n","            maxpool = nn.MaxPool2d(\n","                kernel_size=int(module_def[\"size\"]),\n","                stride=int(module_def[\"stride\"]),\n","                padding=int((kernel_size - 1) // 2),\n","            )\n","            modules.add_module(\"maxpool_%d\" % i, maxpool)\n","\n","        elif module_def[\"type\"] == \"upsample\":\n","            upsample = nn.Upsample(scale_factor=int(module_def[\"stride\"]), mode=\"nearest\")\n","            modules.add_module(\"upsample_%d\" % i, upsample)\n","\n","        elif module_def[\"type\"] == \"route\":\n","            layers = [int(x) for x in module_def[\"layers\"].split(\",\")]\n","            filters = sum([output_filters[layer_i] for layer_i in layers])\n","            modules.add_module(\"route_%d\" % i, EmptyLayer())\n","\n","        elif module_def[\"type\"] == \"shortcut\":\n","            filters = output_filters[int(module_def[\"from\"])]\n","            modules.add_module(\"shortcut_%d\" % i, EmptyLayer())\n","\n","        elif module_def[\"type\"] == \"yolo\":\n","            anchor_idxs = [int(x) for x in module_def[\"mask\"].split(\",\")]\n","            # Extract anchors\n","            anchors = [int(x) for x in module_def[\"anchors\"].split(\",\")]\n","            anchors = [(anchors[i], anchors[i + 1]) for i in range(0, len(anchors), 2)]\n","            anchors = [anchors[i] for i in anchor_idxs]\n","            num_classes = int(module_def[\"classes\"])\n","            img_height = int(hyperparams[\"height\"])\n","            # Define detection layer\n","            yolo_layer = YOLOLayer(anchors, num_classes, img_height)\n","            modules.add_module(\"yolo_%d\" % i, yolo_layer)\n","        # Register module list and number of output filters\n","        module_list.append(modules)\n","        output_filters.append(filters)\n","\n","    return hyperparams, module_list\n","\n","\n","class EmptyLayer(nn.Module):\n","    \"\"\"Placeholder for 'route' and 'shortcut' layers\"\"\"\n","\n","    def __init__(self):\n","        super(EmptyLayer, self).__init__()\n","\n","\n","class YOLOLayer(nn.Module):\n","    \"\"\"Detection layer\"\"\"\n","\n","    def __init__(self, anchors, num_classes, img_dim):\n","        super(YOLOLayer, self).__init__()\n","        self.anchors = anchors\n","        self.num_anchors = len(anchors)\n","        self.num_classes = num_classes\n","        self.bbox_attrs = 5 + num_classes\n","        self.image_dim = img_dim\n","        self.ignore_thres = 0.5\n","        self.lambda_coord = 1\n","\n","        self.mse_loss = nn.MSELoss(size_average=True)  # Coordinate loss\n","        self.bce_loss = nn.BCELoss(size_average=True)  # Confidence loss\n","        self.ce_loss = nn.CrossEntropyLoss()  # Class loss\n","\n","    def forward(self, x, targets=None):\n","        nA = self.num_anchors\n","        nB = x.size(0)\n","        nG = x.size(2)\n","        stride = self.image_dim / nG\n","\n","        # Tensors for cuda support\n","        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\n","        LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor\n","        ByteTensor = torch.cuda.ByteTensor if x.is_cuda else torch.ByteTensor\n","\n","        prediction = x.view(nB, nA, self.bbox_attrs, nG, nG).permute(0, 1, 3, 4, 2).contiguous()\n","\n","        # Get outputs\n","        x = torch.sigmoid(prediction[..., 0])  # Center x\n","        y = torch.sigmoid(prediction[..., 1])  # Center y\n","        w = prediction[..., 2]  # Width\n","        h = prediction[..., 3]  # Height\n","        pred_conf = torch.sigmoid(prediction[..., 4])  # Conf\n","        pred_cls = torch.sigmoid(prediction[..., 5:])  # Cls pred.\n","\n","        # Calculate offsets for each grid\n","        grid_x = torch.arange(nG).repeat(nG, 1).view([1, 1, nG, nG]).type(FloatTensor)\n","        grid_y = torch.arange(nG).repeat(nG, 1).t().view([1, 1, nG, nG]).type(FloatTensor)\n","        scaled_anchors = FloatTensor([(a_w / stride, a_h / stride) for a_w, a_h in self.anchors])\n","        anchor_w = scaled_anchors[:, 0:1].view((1, nA, 1, 1))\n","        anchor_h = scaled_anchors[:, 1:2].view((1, nA, 1, 1))\n","\n","        # Add offset and scale with anchors\n","        pred_boxes = FloatTensor(prediction[..., :4].shape)\n","        pred_boxes[..., 0] = x.data + grid_x\n","        pred_boxes[..., 1] = y.data + grid_y\n","        pred_boxes[..., 2] = torch.exp(w.data) * anchor_w\n","        pred_boxes[..., 3] = torch.exp(h.data) * anchor_h\n","\n","        # Training\n","        if targets is not None:\n","\n","            if x.is_cuda:\n","                self.mse_loss = self.mse_loss.cuda()\n","                self.bce_loss = self.bce_loss.cuda()\n","                self.ce_loss = self.ce_loss.cuda()\n","\n","            nGT, nCorrect, mask, conf_mask, tx, ty, tw, th, tconf, tcls = build_targets(\n","                pred_boxes=pred_boxes.cpu().data,\n","                pred_conf=pred_conf.cpu().data,\n","                pred_cls=pred_cls.cpu().data,\n","                target=targets.cpu().data,\n","                anchors=scaled_anchors.cpu().data,\n","                num_anchors=nA,\n","                num_classes=self.num_classes,\n","                grid_size=nG,\n","                ignore_thres=self.ignore_thres,\n","                img_dim=self.image_dim,\n","            )\n","\n","            nProposals = int((pred_conf > 0.5).sum().item())\n","            recall = float(nCorrect / nGT) if nGT else 1\n","            precision = 0\n","            if nProposals > 0:\n","                precision = float(nCorrect / nProposals)\n","\n","            # Handle masks\n","            mask = Variable(mask.type(ByteTensor))\n","            conf_mask = Variable(conf_mask.type(ByteTensor))\n","\n","            # Handle target variables\n","            tx = Variable(tx.type(FloatTensor), requires_grad=False)\n","            ty = Variable(ty.type(FloatTensor), requires_grad=False)\n","            tw = Variable(tw.type(FloatTensor), requires_grad=False)\n","            th = Variable(th.type(FloatTensor), requires_grad=False)\n","            tconf = Variable(tconf.type(FloatTensor), requires_grad=False)\n","            tcls = Variable(tcls.type(LongTensor), requires_grad=False)\n","\n","            # Get conf mask where gt and where there is no gt\n","            conf_mask_true = mask\n","            conf_mask_false = conf_mask - mask\n","\n","            # Mask outputs to ignore non-existing objects\n","            loss_x = self.mse_loss(x[mask], tx[mask])\n","            loss_y = self.mse_loss(y[mask], ty[mask])\n","            loss_w = self.mse_loss(w[mask], tw[mask])\n","            loss_h = self.mse_loss(h[mask], th[mask])\n","            loss_conf = self.bce_loss(pred_conf[conf_mask_false], tconf[conf_mask_false]) + self.bce_loss(\n","                pred_conf[conf_mask_true], tconf[conf_mask_true]\n","            )\n","            loss_cls = (1 / nB) * self.ce_loss(pred_cls[mask], torch.argmax(tcls[mask], 1))\n","            loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls\n","\n","            return (\n","                loss,\n","                loss_x.item(),\n","                loss_y.item(),\n","                loss_w.item(),\n","                loss_h.item(),\n","                loss_conf.item(),\n","                loss_cls.item(),\n","                recall,\n","                precision,\n","            )\n","\n","        else:\n","            # If not in training phase return predictions\n","            output = torch.cat(\n","                (\n","                    pred_boxes.view(nB, -1, 4) * stride,\n","                    pred_conf.view(nB, -1, 1),\n","                    pred_cls.view(nB, -1, self.num_classes),\n","                ),\n","                -1,\n","            )\n","            return output\n","\n","\n","class Darknet(nn.Module):\n","    \"\"\"YOLOv3 object detection model\"\"\"\n","\n","    def __init__(self, config_path, img_size=416):\n","        super(Darknet, self).__init__()\n","        self.module_defs = parse_model_config(config_path)\n","        self.hyperparams, self.module_list = create_modules(self.module_defs)\n","        self.img_size = img_size\n","        self.seen = 0\n","        self.header_info = np.array([0, 0, 0, self.seen, 0])\n","        self.loss_names = [\"x\", \"y\", \"w\", \"h\", \"conf\", \"cls\", \"recall\", \"precision\"]\n","\n","    def forward(self, x, targets=None):\n","        is_training = targets is not None\n","        output = []\n","        self.losses = defaultdict(float)\n","        layer_outputs = []\n","        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n","            if module_def[\"type\"] in [\"convolutional\", \"upsample\", \"maxpool\"]:\n","                x = module(x)\n","            elif module_def[\"type\"] == \"route\":\n","                layer_i = [int(x) for x in module_def[\"layers\"].split(\",\")]\n","                x = torch.cat([layer_outputs[i] for i in layer_i], 1)\n","            elif module_def[\"type\"] == \"shortcut\":\n","                layer_i = int(module_def[\"from\"])\n","                x = layer_outputs[-1] + layer_outputs[layer_i]\n","            elif module_def[\"type\"] == \"yolo\":\n","                # Train phase: get loss\n","                if is_training:\n","                    x, *losses = module[0](x, targets)\n","                    for name, loss in zip(self.loss_names, losses):\n","                        self.losses[name] += loss\n","                # Test phase: Get detections\n","                else:\n","                    x = module(x)\n","                output.append(x)\n","            layer_outputs.append(x)\n","\n","        self.losses[\"recall\"] /= 3\n","        self.losses[\"precision\"] /= 3\n","        return sum(output) if is_training else torch.cat(output, 1)\n","\n","    def load_weights(self, weights_path):\n","        \"\"\"Parses and loads the weights stored in 'weights_path'\"\"\"\n","\n","        # Open the weights file\n","        fp = open(weights_path, \"rb\")\n","        header = np.fromfile(fp, dtype=np.int32, count=5)  # First five are header values\n","\n","        # Needed to write header when saving weights\n","        self.header_info = header\n","\n","        self.seen = header[3]\n","        weights = np.fromfile(fp, dtype=np.float32)  # The rest are weights\n","        fp.close()\n","\n","        # Establish cutoff for loading backbone weights\n","        cutoff = None\n","        if \"darknet53.conv.74\" in weights_path:\n","            cutoff = 75\n","\n","        ptr = 0\n","        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n","            if i == cutoff:\n","              break\n","            if module_def[\"type\"] == \"convolutional\":\n","                conv_layer = module[0]\n","                if module_def[\"batch_normalize\"]:\n","                    # Load BN bias, weights, running mean and running variance\n","                    bn_layer = module[1]\n","                    num_b = bn_layer.bias.numel()  # Number of biases\n","                    # Bias\n","                    bn_b = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.bias)\n","                    bn_layer.bias.data.copy_(bn_b)\n","                    ptr += num_b\n","                    # Weight\n","                    bn_w = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.weight)\n","                    bn_layer.weight.data.copy_(bn_w)\n","                    ptr += num_b\n","                    # Running Mean\n","                    bn_rm = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.running_mean)\n","                    bn_layer.running_mean.data.copy_(bn_rm)\n","                    ptr += num_b\n","                    # Running Var\n","                    bn_rv = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.running_var)\n","                    bn_layer.running_var.data.copy_(bn_rv)\n","                    ptr += num_b\n","                else:\n","                    # Load conv. bias\n","                    num_b = conv_layer.bias.numel()\n","                    conv_b = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(conv_layer.bias)\n","                    conv_layer.bias.data.copy_(conv_b)\n","                    ptr += num_b\n","                # Load conv. weights\n","                num_w = conv_layer.weight.numel()\n","                conv_w = torch.from_numpy(weights[ptr : ptr + num_w]).view_as(conv_layer.weight)\n","                conv_layer.weight.data.copy_(conv_w)\n","                ptr += num_w\n","\n","    \"\"\"\n","        @:param path    - path of the new weights file\n","        @:param cutoff  - save layers between 0 and cutoff (cutoff = -1 -> all are saved)\n","    \"\"\"\n","\n","    def save_weights(self, path, cutoff=-1):\n","\n","        fp = open(path, \"wb\")\n","        self.header_info[3] = self.seen\n","        self.header_info.tofile(fp)\n","\n","        # Iterate through layers\n","        for i, (module_def, module) in enumerate(zip(self.module_defs[:cutoff], self.module_list[:cutoff])):\n","            if module_def[\"type\"] == \"convolutional\":\n","                conv_layer = module[0]\n","                # If batch norm, load bn first\n","                if module_def[\"batch_normalize\"]:\n","                    bn_layer = module[1]\n","                    bn_layer.bias.data.cpu().numpy().tofile(fp)\n","                    bn_layer.weight.data.cpu().numpy().tofile(fp)\n","                    bn_layer.running_mean.data.cpu().numpy().tofile(fp)\n","                    bn_layer.running_var.data.cpu().numpy().tofile(fp)\n","                # Load conv bias\n","                else:\n","                    conv_layer.bias.data.cpu().numpy().tofile(fp)\n","                # Load conv weights\n","                conv_layer.weight.data.cpu().numpy().tofile(fp)\n","\n","        fp.close()"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"psne_OmKYTAf"},"source":["## Train"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HLpqCqt0Xddb","outputId":"ea2c768f-dcde-46fe-a01c-81007431a2ea"},"source":["parser = argparse.ArgumentParser()\n","parser.add_argument(\"--epochs\", type=int, default=20, help=\"number of epochs\")\n","parser.add_argument(\"--image_folder\", type=str, default=\"/content/drive/Shareddrives/COS-470/Gestures/data/gestures_dataset/images\", help=\"path to dataset\")\n","parser.add_argument(\"--batch_size\", type=int, default=16, help=\"size of each image batch\")\n","parser.add_argument(\"--model_config_path\", type=str, default=\"/content/drive/Shareddrives/COS-470/Gestures/config/yolov3-custom.cfg\", help=\"path to model config file\")\n","parser.add_argument(\"--data_config_path\", type=str, default=\"/content/drive/Shareddrives/COS-470/Gestures/config/coco.data\", help=\"path to data config file\")\n","parser.add_argument(\"--weights_path\", type=str, default=\"/content/drive/Shareddrives/COS-470/Gestures/weights/darknet53.conv.74.weights\", help=\"path to weights file\")\n","parser.add_argument(\"--class_path\", type=str, default=\"/content/drive/Shareddrives/COS-470/Gestures/config/coco.names\", help=\"path to class label file\")\n","parser.add_argument(\"--conf_thres\", type=float, default=0.8, help=\"object confidence threshold\")\n","parser.add_argument(\"--nms_thres\", type=float, default=0.4, help=\"iou thresshold for non-maximum suppression\")\n","parser.add_argument(\"--n_cpu\", type=int, default=0, help=\"number of cpu threads to use during batch generation\")\n","parser.add_argument(\"--img_size\", type=int, default=416, help=\"size of each image dimension\")\n","parser.add_argument(\"--checkpoint_interval\", type=int, default=1, help=\"interval between saving model weights\")\n","parser.add_argument(\"--checkpoint_dir\", type=str, default=\"/content/drive/Shareddrives/COS-470/Gestures/checkpoints\", help=\"directory where model checkpoints are saved\")\n","parser.add_argument(\"--use_cuda\", type=bool, default=True, help=\"whether to use cuda if available\")\n","opt = parser.parse_args(\"\")\n","print(opt)\n","\n","cuda = torch.cuda.is_available() and opt.use_cuda\n","\n","classes = load_classes(opt.class_path)\n","\n","# Get data configuration\n","data_config = parse_data_config(opt.data_config_path)\n","train_path = data_config[\"train\"]\n","valid_path = data_config[\"valid\"]\n","test_path = data_config[\"test\"]\n","\n","# Get hyper parameters\n","hyperparams = parse_model_config(opt.model_config_path)[0]\n","learning_rate = 0.1 #float(hyperparams[\"learning_rate\"])\n","momentum = float(hyperparams[\"momentum\"])\n","decay = float(hyperparams[\"decay\"])\n","burn_in = int(hyperparams[\"burn_in\"])\n","\n","# Initiate model\n","model = Darknet(opt.model_config_path)\n","model.load_weights(opt.weights_path)\n","\n","if cuda:\n","    model = model.cuda()\n","\n","model.train()\n","\n","# Get dataloader\n","dataloader = torch.utils.data.DataLoader(\n","    ListDataset(train_path), batch_size=opt.batch_size, shuffle=False, num_workers=opt.n_cpu\n",")\n","\n","# Get validation dataloader\n","valid_dataloader = torch.utils.data.DataLoader(\n","    ListDataset(valid_path), batch_size=19, shuffle=False, num_workers=opt.n_cpu\n",")\n","\n","Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n","\n","optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n","\n","losses = []\n","valid_losses = []\n","\n","for epoch in range(opt.epochs):\n","    losses_temp = []\n","\n","    for batch_i, (_, imgs, targets) in enumerate(dataloader):\n","        imgs = torch.tensor(imgs).cuda()\n","        targets = torch.tensor(targets, requires_grad=False).cuda()\n","\n","        optimizer.zero_grad()\n","\n","        loss = model(imgs, targets)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        losses_temp.append(loss.item())\n","\n","        print(\n","            \"[Epoch %d/%d, Batch %d/%d] [Losses: x %f, y %f, w %f, h %f, conf %f, cls %f, total %f, recall: %.5f, precision: %.5f]\"\n","            % (\n","                epoch,\n","                opt.epochs,\n","                batch_i,\n","                len(dataloader),\n","                model.losses[\"x\"],\n","                model.losses[\"y\"],\n","                model.losses[\"w\"],\n","                model.losses[\"h\"],\n","                model.losses[\"conf\"],\n","                model.losses[\"cls\"],\n","                loss.item(),\n","                model.losses[\"recall\"],\n","                model.losses[\"precision\"],\n","                #compute_ap(model.losses[\"recall\"], model.losses[\"precision\"]),\n","            )\n","        )\n","\n","        model.seen += imgs.size(0)\n","\n","    losses.append(np.average(losses_temp))\n","\n","    if epoch % opt.checkpoint_interval == 0:\n","        #model.save_weights(\"%s/%d.weights\" % (opt.checkpoint_dir, epoch))\n","        _ = \"don't save weights\"\n","\n","    with torch.no_grad():\n","      valid_losses_temp = []\n","      for valid_batch_i, (_, valid_imgs, valid_targets) in enumerate(valid_dataloader):\n","        valid_imgs = torch.tensor(valid_imgs).cuda()\n","        valid_targets = torch.tensor(valid_targets, requires_grad=False).cuda()\n","\n","        valid_loss = model(valid_imgs, valid_targets)\n","        valid_losses_temp.append(valid_loss.item())\n","\n","      valid_losses.append(np.average(valid_losses_temp))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Namespace(batch_size=16, checkpoint_dir='/content/drive/Shareddrives/COS-470/Gestures/checkpoints', checkpoint_interval=1, class_path='/content/drive/Shareddrives/COS-470/Gestures/config/coco.names', conf_thres=0.8, data_config_path='/content/drive/Shareddrives/COS-470/Gestures/config/coco.data', epochs=20, image_folder='/content/drive/Shareddrives/COS-470/Gestures/data/gestures_dataset/images', img_size=416, model_config_path='/content/drive/Shareddrives/COS-470/Gestures/config/yolov3-custom.cfg', n_cpu=0, nms_thres=0.4, use_cuda=True, weights_path='/content/drive/Shareddrives/COS-470/Gestures/weights/darknet53.conv.74.weights')\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n","  warnings.warn(warning.format(ret))\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:179: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:30.)\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:180: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:30.)\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:181: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:30.)\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:182: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:30.)\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:183: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:30.)\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:184: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:30.)\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:186: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:30.)\n"],"name":"stderr"},{"output_type":"stream","text":["[Epoch 0/20, Batch 0/20] [Losses: x 0.326108, y 0.272827, w 2.930664, h 6.757715, conf 4.323981, cls 0.257811, total 14.869106, recall: 0.02083, precision: 0.00009]\n","[Epoch 0/20, Batch 1/20] [Losses: x 0.225668, y 0.303113, w 3.889133, h 5.936562, conf 3.360882, cls 0.258379, total 13.973736, recall: 0.04167, precision: 0.00011]\n","[Epoch 0/20, Batch 2/20] [Losses: x 0.289858, y 0.272358, w 70.976709, h 53.386628, conf 4.005294, cls 0.254920, total 129.185776, recall: 0.00000, precision: 0.00000]\n","[Epoch 0/20, Batch 3/20] [Losses: x 0.292329, y 0.229905, w 26.501021, h 17.116848, conf 3.733010, cls 0.272386, total 48.145500, recall: 0.00000, precision: 0.00000]\n","[Epoch 0/20, Batch 4/20] [Losses: x 0.240922, y 0.254053, w 3.566687, h 8.061021, conf 3.711966, cls 0.261057, total 16.095707, recall: 0.02083, precision: 0.00005]\n","[Epoch 0/20, Batch 5/20] [Losses: x 0.331198, y 0.273443, w 6.788627, h 12.499149, conf 3.652124, cls 0.265147, total 23.809689, recall: 0.02083, precision: 0.00001]\n","[Epoch 0/20, Batch 6/20] [Losses: x 0.183078, y 0.281098, w 1.191710, h 1.155586, conf 3.698031, cls 0.257741, total 6.767243, recall: 0.06250, precision: 0.00049]\n","[Epoch 0/20, Batch 7/20] [Losses: x 0.242453, y 0.306809, w 4.870180, h 21.707814, conf 4.236207, cls 0.260528, total 31.623989, recall: 0.06250, precision: 0.00029]\n","[Epoch 0/20, Batch 8/20] [Losses: x 0.326010, y 0.261083, w 1.363620, h 1.847165, conf 3.475903, cls 0.263944, total 7.537724, recall: 0.04167, precision: 0.00006]\n","[Epoch 0/20, Batch 9/20] [Losses: x 0.257112, y 0.272014, w 1.293559, h 1.866778, conf 3.631614, cls 0.261157, total 7.582233, recall: 0.06250, precision: 0.00019]\n","[Epoch 0/20, Batch 10/20] [Losses: x 0.225571, y 0.233697, w 1.261208, h 0.801571, conf 3.464119, cls 0.262463, total 6.248630, recall: 0.08333, precision: 0.00031]\n","[Epoch 0/20, Batch 11/20] [Losses: x 0.246798, y 0.444411, w 28.727045, h 24.816814, conf 3.122947, cls 0.260873, total 57.618893, recall: 0.10417, precision: 0.00040]\n","[Epoch 0/20, Batch 12/20] [Losses: x 0.203115, y 0.234570, w 8.572265, h 6.312451, conf 3.178146, cls 0.255614, total 18.756161, recall: 0.10417, precision: 0.00041]\n","[Epoch 0/20, Batch 13/20] [Losses: x 0.262183, y 0.310622, w 3.379115, h 3.869179, conf 3.003027, cls 0.256090, total 11.080217, recall: 0.14583, precision: 0.00037]\n","[Epoch 0/20, Batch 14/20] [Losses: x 0.215400, y 0.295062, w 2.471498, h 5.045555, conf 3.169185, cls 0.259784, total 11.456484, recall: 0.25000, precision: 0.00049]\n","[Epoch 0/20, Batch 15/20] [Losses: x 0.250629, y 0.206900, w 1.750663, h 0.269086, conf 3.253665, cls 0.254762, total 5.985704, recall: 0.10417, precision: 0.00015]\n","[Epoch 0/20, Batch 16/20] [Losses: x 0.277526, y 0.245203, w 1.237122, h 0.293421, conf 3.459992, cls 0.261919, total 5.775184, recall: 0.00000, precision: 0.00000]\n","[Epoch 0/20, Batch 17/20] [Losses: x 0.339939, y 0.239613, w 0.611794, h 0.541554, conf 3.097630, cls 0.266718, total 5.097249, recall: 0.00000, precision: 0.00000]\n","[Epoch 0/20, Batch 18/20] [Losses: x 0.251898, y 0.232216, w 0.437611, h 0.327166, conf 3.079214, cls 0.266832, total 4.594937, recall: 0.00000, precision: 0.00000]\n","[Epoch 0/20, Batch 19/20] [Losses: x 0.236281, y 0.345241, w 0.463062, h 0.535385, conf 2.945396, cls 0.722718, total 5.248084, recall: 0.00000, precision: 0.00000]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"],"name":"stderr"},{"output_type":"stream","text":["[Epoch 1/20, Batch 0/20] [Losses: x 0.270081, y 0.252573, w 0.968753, h 4.871916, conf 3.102200, cls 0.257989, total 9.723511, recall: 0.08333, precision: 0.00013]\n","[Epoch 1/20, Batch 1/20] [Losses: x 0.212570, y 0.281257, w 0.809772, h 0.886483, conf 2.944233, cls 0.258610, total 5.392924, recall: 0.16667, precision: 0.00052]\n","[Epoch 1/20, Batch 2/20] [Losses: x 0.203334, y 0.238966, w 0.563277, h 0.939848, conf 2.810100, cls 0.255726, total 5.011251, recall: 0.22917, precision: 0.00039]\n","[Epoch 1/20, Batch 3/20] [Losses: x 0.267546, y 0.175161, w 5.845949, h 5.353848, conf 3.421760, cls 0.265306, total 15.329569, recall: 0.16667, precision: 0.00041]\n","[Epoch 1/20, Batch 4/20] [Losses: x 0.194569, y 0.238906, w 1.098721, h 2.764330, conf 2.907182, cls 0.258123, total 7.461830, recall: 0.12500, precision: 0.00031]\n","[Epoch 1/20, Batch 5/20] [Losses: x 0.253478, y 0.284569, w 1.978110, h 4.702753, conf 2.930738, cls 0.266652, total 10.416299, recall: 0.16667, precision: 0.00037]\n","[Epoch 1/20, Batch 6/20] [Losses: x 0.172243, y 0.276364, w 0.423871, h 0.819381, conf 2.752361, cls 0.257602, total 4.701822, recall: 0.33333, precision: 0.00077]\n","[Epoch 1/20, Batch 7/20] [Losses: x 0.246133, y 0.270095, w 4.752135, h 2.799924, conf 3.052822, cls 0.259542, total 11.380650, recall: 0.14583, precision: 0.00029]\n","[Epoch 1/20, Batch 8/20] [Losses: x 0.319491, y 0.267882, w 0.634191, h 0.154449, conf 2.612253, cls 0.261290, total 4.249557, recall: 0.20833, precision: 0.00038]\n","[Epoch 1/20, Batch 9/20] [Losses: x 0.305190, y 0.285581, w 1.149399, h 0.610917, conf 2.820064, cls 0.260257, total 5.431407, recall: 0.22917, precision: 0.00050]\n","[Epoch 1/20, Batch 10/20] [Losses: x 0.209630, y 0.248611, w 0.722542, h 0.310727, conf 2.758956, cls 0.260548, total 4.511015, recall: 0.12500, precision: 0.00035]\n","[Epoch 1/20, Batch 11/20] [Losses: x 0.238019, y 0.350023, w 1.249860, h 0.933796, conf 2.819938, cls 0.260691, total 5.852326, recall: 0.08333, precision: 0.00022]\n","[Epoch 1/20, Batch 12/20] [Losses: x 0.229925, y 0.195604, w 1.424895, h 0.846086, conf 2.822377, cls 0.255354, total 5.774240, recall: 0.08333, precision: 0.00022]\n","[Epoch 1/20, Batch 13/20] [Losses: x 0.242963, y 0.297115, w 0.880761, h 0.438391, conf 2.717810, cls 0.257830, total 4.834870, recall: 0.22917, precision: 0.00039]\n","[Epoch 1/20, Batch 14/20] [Losses: x 0.171550, y 0.274061, w 1.002714, h 2.904066, conf 2.761838, cls 0.262088, total 7.376316, recall: 0.10417, precision: 0.00019]\n","[Epoch 1/20, Batch 15/20] [Losses: x 0.239738, y 0.224035, w 1.347737, h 0.446852, conf 2.511121, cls 0.260884, total 5.030365, recall: 0.00000, precision: 0.00000]\n","[Epoch 1/20, Batch 16/20] [Losses: x 0.242723, y 0.242018, w 1.324314, h 0.798402, conf 2.995816, cls 0.267274, total 5.870547, recall: 0.00000, precision: 0.00000]\n","[Epoch 1/20, Batch 17/20] [Losses: x 0.308394, y 0.210973, w 1.236989, h 1.231975, conf 2.434746, cls 0.272572, total 5.695650, recall: 0.02083, precision: 0.00007]\n","[Epoch 1/20, Batch 18/20] [Losses: x 0.264805, y 0.208601, w 0.953498, h 0.395450, conf 2.543574, cls 0.245541, total 4.611470, recall: 0.29167, precision: 0.00087]\n","[Epoch 1/20, Batch 19/20] [Losses: x 0.217792, y 0.325296, w 0.660968, h 0.178388, conf 2.329505, cls 0.635727, total 4.347676, recall: 0.61111, precision: 0.00124]\n","[Epoch 2/20, Batch 0/20] [Losses: x 0.246705, y 0.254451, w 0.394798, h 1.244745, conf 2.850252, cls 0.263471, total 5.254423, recall: 0.04167, precision: 0.00008]\n","[Epoch 2/20, Batch 1/20] [Losses: x 0.212955, y 0.274905, w 0.426256, h 0.633787, conf 2.781898, cls 0.249723, total 4.579523, recall: 0.43750, precision: 0.00086]\n","[Epoch 2/20, Batch 2/20] [Losses: x 0.202746, y 0.252632, w 0.292168, h 0.843697, conf 2.625447, cls 0.257328, total 4.474017, recall: 0.12500, precision: 0.00026]\n","[Epoch 2/20, Batch 3/20] [Losses: x 0.256458, y 0.193302, w 0.647837, h 1.311910, conf 2.842330, cls 0.262248, total 5.514086, recall: 0.25000, precision: 0.00051]\n","[Epoch 2/20, Batch 4/20] [Losses: x 0.235817, y 0.257690, w 0.584132, h 0.848776, conf 2.553478, cls 0.258341, total 4.738233, recall: 0.20833, precision: 0.00040]\n","[Epoch 2/20, Batch 5/20] [Losses: x 0.248074, y 0.256634, w 0.598772, h 0.786978, conf 2.658191, cls 0.261166, total 4.809815, recall: 0.20833, precision: 0.00042]\n","[Epoch 2/20, Batch 6/20] [Losses: x 0.174468, y 0.267241, w 0.344723, h 0.737965, conf 2.557905, cls 0.254112, total 4.336413, recall: 0.31250, precision: 0.00069]\n","[Epoch 2/20, Batch 7/20] [Losses: x 0.229141, y 0.237562, w 1.100699, h 1.188923, conf 2.948495, cls 0.259198, total 5.964017, recall: 0.12500, precision: 0.00027]\n","[Epoch 2/20, Batch 8/20] [Losses: x 0.283456, y 0.263137, w 0.673751, h 0.659436, conf 2.392021, cls 0.260448, total 4.532249, recall: 0.22917, precision: 0.00043]\n","[Epoch 2/20, Batch 9/20] [Losses: x 0.307008, y 0.278378, w 0.786464, h 1.708852, conf 2.651619, cls 0.258099, total 5.990422, recall: 0.20833, precision: 0.00048]\n","[Epoch 2/20, Batch 10/20] [Losses: x 0.222296, y 0.248224, w 0.542604, h 0.276470, conf 2.569939, cls 0.261788, total 4.121322, recall: 0.14583, precision: 0.00034]\n","[Epoch 2/20, Batch 11/20] [Losses: x 0.230713, y 0.352994, w 0.958907, h 0.210294, conf 2.559621, cls 0.261707, total 4.574237, recall: 0.14583, precision: 0.00030]\n","[Epoch 2/20, Batch 12/20] [Losses: x 0.204947, y 0.210328, w 0.921896, h 0.227431, conf 2.503995, cls 0.258347, total 4.326943, recall: 0.14583, precision: 0.00033]\n","[Epoch 2/20, Batch 13/20] [Losses: x 0.225576, y 0.283824, w 1.072901, h 0.386757, conf 2.542891, cls 0.260152, total 4.772100, recall: 0.16667, precision: 0.00033]\n","[Epoch 2/20, Batch 14/20] [Losses: x 0.175909, y 0.294967, w 0.599469, h 0.271218, conf 2.593843, cls 0.264792, total 4.200200, recall: 0.04167, precision: 0.00012]\n","[Epoch 2/20, Batch 15/20] [Losses: x 0.236098, y 0.216778, w 0.626059, h 0.190038, conf 2.530265, cls 0.270431, total 4.069668, recall: 0.02083, precision: 0.00003]\n","[Epoch 2/20, Batch 16/20] [Losses: x 0.245691, y 0.244905, w 0.570688, h 0.243157, conf 3.129905, cls 0.270527, total 4.704872, recall: 0.00000, precision: 0.00000]\n","[Epoch 2/20, Batch 17/20] [Losses: x 0.322469, y 0.207309, w 0.490953, h 0.248953, conf 2.412408, cls 0.275199, total 3.957291, recall: 0.06250, precision: 0.00011]\n","[Epoch 2/20, Batch 18/20] [Losses: x 0.257632, y 0.210469, w 0.299170, h 0.149239, conf 2.560199, cls 0.229177, total 3.705887, recall: 0.77083, precision: 0.00135]\n","[Epoch 2/20, Batch 19/20] [Losses: x 0.234505, y 0.326492, w 0.166239, h 0.110767, conf 2.352021, cls 0.578423, total 3.768446, recall: 0.94444, precision: 0.00169]\n","[Epoch 3/20, Batch 0/20] [Losses: x 0.236104, y 0.261865, w 0.403048, h 0.695834, conf 2.668186, cls 0.268260, total 4.533297, recall: 0.02083, precision: 0.00006]\n","[Epoch 3/20, Batch 1/20] [Losses: x 0.210035, y 0.276332, w 0.218452, h 0.412594, conf 2.544786, cls 0.240738, total 3.902936, recall: 0.52083, precision: 0.00097]\n","[Epoch 3/20, Batch 2/20] [Losses: x 0.206917, y 0.248139, w 0.302692, h 0.521442, conf 2.367139, cls 0.259933, total 3.906262, recall: 0.14583, precision: 0.00030]\n","[Epoch 3/20, Batch 3/20] [Losses: x 0.260270, y 0.181615, w 0.820202, h 0.845150, conf 2.597399, cls 0.259047, total 4.963683, recall: 0.20833, precision: 0.00040]\n","[Epoch 3/20, Batch 4/20] [Losses: x 0.196417, y 0.237974, w 0.588465, h 0.138350, conf 2.345610, cls 0.259671, total 3.766486, recall: 0.22917, precision: 0.00046]\n","[Epoch 3/20, Batch 5/20] [Losses: x 0.246655, y 0.251057, w 0.492110, h 0.255008, conf 2.373388, cls 0.261372, total 3.879590, recall: 0.14583, precision: 0.00031]\n","[Epoch 3/20, Batch 6/20] [Losses: x 0.175018, y 0.268489, w 0.287310, h 0.189894, conf 2.286556, cls 0.252694, total 3.459962, recall: 0.35417, precision: 0.00070]\n","[Epoch 3/20, Batch 7/20] [Losses: x 0.225993, y 0.224527, w 0.673473, h 0.597665, conf 2.648044, cls 0.258479, total 4.628181, recall: 0.20833, precision: 0.00037]\n","[Epoch 3/20, Batch 8/20] [Losses: x 0.300343, y 0.249626, w 0.600780, h 0.554906, conf 2.301757, cls 0.261501, total 4.268913, recall: 0.10417, precision: 0.00030]\n","[Epoch 3/20, Batch 9/20] [Losses: x 0.281519, y 0.261425, w 0.439250, h 0.785565, conf 2.616580, cls 0.256200, total 4.640539, recall: 0.27083, precision: 0.00054]\n","[Epoch 3/20, Batch 10/20] [Losses: x 0.219313, y 0.251736, w 0.561981, h 0.241029, conf 2.324337, cls 0.263304, total 3.861700, recall: 0.14583, precision: 0.00033]\n","[Epoch 3/20, Batch 11/20] [Losses: x 0.226882, y 0.347236, w 1.006636, h 0.166968, conf 2.364323, cls 0.262407, total 4.374452, recall: 0.14583, precision: 0.00030]\n","[Epoch 3/20, Batch 12/20] [Losses: x 0.231269, y 0.198181, w 1.063890, h 0.256147, conf 2.302724, cls 0.259030, total 4.311240, recall: 0.12500, precision: 0.00032]\n","[Epoch 3/20, Batch 13/20] [Losses: x 0.235375, y 0.286465, w 1.188412, h 0.231120, conf 2.358773, cls 0.261064, total 4.561209, recall: 0.20833, precision: 0.00041]\n","[Epoch 3/20, Batch 14/20] [Losses: x 0.168291, y 0.278415, w 0.792489, h 0.201207, conf 2.420278, cls 0.267362, total 4.128042, recall: 0.06250, precision: 0.00016]\n","[Epoch 3/20, Batch 15/20] [Losses: x 0.232928, y 0.217819, w 0.522651, h 0.180713, conf 2.357271, cls 0.277524, total 3.788906, recall: 0.02083, precision: 0.00004]\n","[Epoch 3/20, Batch 16/20] [Losses: x 0.236833, y 0.242739, w 0.485783, h 0.302997, conf 3.056197, cls 0.273320, total 4.597868, recall: 0.00000, precision: 0.00000]\n","[Epoch 3/20, Batch 17/20] [Losses: x 0.309034, y 0.203530, w 0.385625, h 0.241608, conf 2.290084, cls 0.279234, total 3.709116, recall: 0.04167, precision: 0.00008]\n","[Epoch 3/20, Batch 18/20] [Losses: x 0.255497, y 0.207078, w 0.277967, h 0.095541, conf 2.442933, cls 0.219162, total 3.498178, recall: 0.79167, precision: 0.00150]\n","[Epoch 3/20, Batch 19/20] [Losses: x 0.220755, y 0.326279, w 0.213062, h 0.046558, conf 2.209532, cls 0.543869, total 3.560055, recall: 1.00000, precision: 0.00194]\n","[Epoch 4/20, Batch 0/20] [Losses: x 0.242908, y 0.250822, w 0.371185, h 0.654065, conf 2.549919, cls 0.268907, total 4.337807, recall: 0.06250, precision: 0.00012]\n","[Epoch 4/20, Batch 1/20] [Losses: x 0.213853, y 0.274909, w 0.561903, h 0.734419, conf 2.474955, cls 0.238856, total 4.498897, recall: 0.35417, precision: 0.00091]\n","[Epoch 4/20, Batch 2/20] [Losses: x 0.202028, y 0.249670, w 0.274193, h 0.789070, conf 2.265607, cls 0.262140, total 4.042709, recall: 0.10417, precision: 0.00026]\n","[Epoch 4/20, Batch 3/20] [Losses: x 0.250807, y 0.171635, w 0.427126, h 0.621276, conf 2.594655, cls 0.255472, total 4.320971, recall: 0.22917, precision: 0.00055]\n","[Epoch 4/20, Batch 4/20] [Losses: x 0.239519, y 0.261519, w 0.352681, h 0.385348, conf 2.268799, cls 0.259257, total 3.767124, recall: 0.27083, precision: 0.00062]\n","[Epoch 4/20, Batch 5/20] [Losses: x 0.250942, y 0.251867, w 0.136224, h 0.311347, conf 2.286803, cls 0.261553, total 3.498736, recall: 0.22917, precision: 0.00049]\n","[Epoch 4/20, Batch 6/20] [Losses: x 0.174434, y 0.262991, w 0.458593, h 0.305008, conf 2.203119, cls 0.251305, total 3.655451, recall: 0.27083, precision: 0.00064]\n","[Epoch 4/20, Batch 7/20] [Losses: x 0.217554, y 0.224052, w 0.511126, h 0.470367, conf 2.593670, cls 0.258944, total 4.275714, recall: 0.14583, precision: 0.00040]\n","[Epoch 4/20, Batch 8/20] [Losses: x 0.286479, y 0.252612, w 0.882436, h 0.622028, conf 2.196146, cls 0.261333, total 4.501034, recall: 0.10417, precision: 0.00033]\n","[Epoch 4/20, Batch 9/20] [Losses: x 0.259523, y 0.261374, w 0.301266, h 0.733081, conf 2.413699, cls 0.255764, total 4.224707, recall: 0.31250, precision: 0.00073]\n","[Epoch 4/20, Batch 10/20] [Losses: x 0.219923, y 0.247939, w 0.258107, h 0.439819, conf 2.171516, cls 0.263829, total 3.601133, recall: 0.20833, precision: 0.00045]\n","[Epoch 4/20, Batch 11/20] [Losses: x 0.228392, y 0.347839, w 0.471635, h 0.216599, conf 2.212825, cls 0.264220, total 3.741510, recall: 0.20833, precision: 0.00050]\n","[Epoch 4/20, Batch 12/20] [Losses: x 0.197009, y 0.191648, w 0.558252, h 0.274610, conf 2.152824, cls 0.259836, total 3.634179, recall: 0.20833, precision: 0.00050]\n","[Epoch 4/20, Batch 13/20] [Losses: x 0.227516, y 0.287851, w 0.794382, h 0.279186, conf 2.217817, cls 0.261782, total 4.068534, recall: 0.20833, precision: 0.00050]\n","[Epoch 4/20, Batch 14/20] [Losses: x 0.177901, y 0.278740, w 0.527314, h 0.164838, conf 2.247080, cls 0.269473, total 3.665346, recall: 0.10417, precision: 0.00027]\n","[Epoch 4/20, Batch 15/20] [Losses: x 0.233405, y 0.214994, w 0.603550, h 0.129888, conf 2.237954, cls 0.281761, total 3.701551, recall: 0.02083, precision: 0.00006]\n","[Epoch 4/20, Batch 16/20] [Losses: x 0.240418, y 0.242373, w 0.518818, h 0.205256, conf 3.016296, cls 0.275341, total 4.498502, recall: 0.00000, precision: 0.00000]\n","[Epoch 4/20, Batch 17/20] [Losses: x 0.316117, y 0.203033, w 0.284548, h 0.129932, conf 2.206157, cls 0.282975, total 3.422762, recall: 0.04167, precision: 0.00008]\n","[Epoch 4/20, Batch 18/20] [Losses: x 0.257565, y 0.207072, w 0.248152, h 0.066428, conf 2.337746, cls 0.211887, total 3.328851, recall: 0.77083, precision: 0.00164]\n","[Epoch 4/20, Batch 19/20] [Losses: x 0.225032, y 0.325146, w 0.120088, h 0.028063, conf 2.115831, cls 0.522077, total 3.336239, recall: 1.00000, precision: 0.00225]\n","[Epoch 5/20, Batch 0/20] [Losses: x 0.242172, y 0.252782, w 0.384105, h 0.791371, conf 2.540213, cls 0.271538, total 4.482181, recall: 0.06250, precision: 0.00020]\n","[Epoch 5/20, Batch 1/20] [Losses: x 0.209182, y 0.274635, w 0.493422, h 0.715187, conf 2.395699, cls 0.235009, total 4.323133, recall: 0.39583, precision: 0.00114]\n","[Epoch 5/20, Batch 2/20] [Losses: x 0.205390, y 0.245262, w 0.269587, h 0.808903, conf 2.152996, cls 0.263584, total 3.945722, recall: 0.14583, precision: 0.00041]\n","[Epoch 5/20, Batch 3/20] [Losses: x 0.249061, y 0.172046, w 0.431316, h 0.641438, conf 2.519935, cls 0.254343, total 4.268139, recall: 0.20833, precision: 0.00057]\n","[Epoch 5/20, Batch 4/20] [Losses: x 0.193132, y 0.232767, w 0.225020, h 0.462594, conf 2.215505, cls 0.259022, total 3.588041, recall: 0.22917, precision: 0.00059]\n","[Epoch 5/20, Batch 5/20] [Losses: x 0.246277, y 0.243318, w 0.227311, h 0.367951, conf 2.207692, cls 0.262086, total 3.554636, recall: 0.16667, precision: 0.00050]\n","[Epoch 5/20, Batch 6/20] [Losses: x 0.175530, y 0.262773, w 0.229258, h 0.136243, conf 2.118804, cls 0.251281, total 3.173888, recall: 0.33333, precision: 0.00093]\n","[Epoch 5/20, Batch 7/20] [Losses: x 0.224520, y 0.223253, w 0.376442, h 0.418192, conf 2.537810, cls 0.259310, total 4.039527, recall: 0.18750, precision: 0.00052]\n","[Epoch 5/20, Batch 8/20] [Losses: x 0.299508, y 0.249636, w 0.434000, h 0.286362, conf 2.146891, cls 0.261565, total 3.677963, recall: 0.20833, precision: 0.00047]\n","[Epoch 5/20, Batch 9/20] [Losses: x 0.262102, y 0.260992, w 0.298619, h 0.495430, conf 2.284162, cls 0.255425, total 3.856729, recall: 0.31250, precision: 0.00084]\n","[Epoch 5/20, Batch 10/20] [Losses: x 0.217132, y 0.249829, w 0.327685, h 0.381730, conf 2.091770, cls 0.263087, total 3.531234, recall: 0.20833, precision: 0.00049]\n","[Epoch 5/20, Batch 11/20] [Losses: x 0.224161, y 0.349242, w 0.496627, h 0.403068, conf 2.161317, cls 0.264654, total 3.899068, recall: 0.16667, precision: 0.00045]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0UiYObfq0oxU"},"source":["x = np.linspace(1, opt.epochs, opt.epochs)\n","\n","plt.plot(x, losses, x, valid_losses)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A1ZG7NCAlkHP"},"source":["## Detect"]},{"cell_type":"markdown","metadata":{"id":"1ra0NzLcLV_W"},"source":["Below code borrowed from: https://github.com/eriklindernoren/PyTorch-YOLOv3"]},{"cell_type":"code","metadata":{"id":"bpVhAdPjLQ7X"},"source":["def box_iou(box1, box2):\n","    # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n","    \"\"\"\n","    Return intersection-over-union (Jaccard index) of boxes.\n","    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n","    Arguments:\n","        box1 (Tensor[N, 4])\n","        box2 (Tensor[M, 4])\n","    Returns:\n","        iou (Tensor[N, M]): the NxM matrix containing the pairwise\n","            IoU values for every element in boxes1 and boxes2\n","    \"\"\"\n","\n","    def box_area(box):\n","        # box = 4xn\n","        return (box[2] - box[0]) * (box[3] - box[1])\n","\n","    area1 = box_area(box1.T)\n","    area2 = box_area(box2.T)\n","\n","    # inter(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n","    inter = (torch.min(box1[:, None, 2:], box2[:, 2:]) -\n","             torch.max(box1[:, None, :2], box2[:, :2])).clamp(0).prod(2)\n","    # iou = inter / (area1 + area2 - inter)\n","    return inter / (area1[:, None] + area2 - inter)\n","\n","def xywh2xyxy(x):\n","    y = x.new(x.shape)\n","    y[..., 0] = x[..., 0] - x[..., 2] / 2\n","    y[..., 1] = x[..., 1] - x[..., 3] / 2\n","    y[..., 2] = x[..., 0] + x[..., 2] / 2\n","    y[..., 3] = x[..., 1] + x[..., 3] / 2\n","    return y\n","\n","def rescale_boxes(boxes, current_dim, original_shape):\n","    \"\"\"\n","    Rescales bounding boxes to the original shape\n","    \"\"\"\n","    orig_h, orig_w = original_shape\n","\n","    # The amount of padding that was added\n","    pad_x = max(orig_h - orig_w, 0) * (current_dim / max(original_shape))\n","    pad_y = max(orig_w - orig_h, 0) * (current_dim / max(original_shape))\n","\n","    # Image height and width after padding is removed\n","    unpad_h = current_dim - pad_y\n","    unpad_w = current_dim - pad_x\n","\n","    # Rescale bounding boxes to dimension of original image\n","    boxes[:, 0] = ((boxes[:, 0] - pad_x // 2) / unpad_w) * orig_w\n","    boxes[:, 1] = ((boxes[:, 1] - pad_y // 2) / unpad_h) * orig_h\n","    boxes[:, 2] = ((boxes[:, 2] - pad_x // 2) / unpad_w) * orig_w\n","    boxes[:, 3] = ((boxes[:, 3] - pad_y // 2) / unpad_h) * orig_h\n","    return boxes\n","\n","\n","def draw_and_save_output_image(image_path, detections, img_size, output_path, classes):\n","    \"\"\"Draws detections in output image and stores this.\n","    :param image_path: Path to input image\n","    :type image_path: str\n","    :param detections: List of detections on image\n","    :type detections: [Tensor]\n","    :param img_size: Size of each image dimension for yolo\n","    :type img_size: int\n","    :param output_path: Path of output directory\n","    :type output_path: str\n","    :param classes: List of class names\n","    :type classes: [str]\n","    \"\"\"\n","    # Create plot\n","    img = np.array(Image.open(image_path))\n","    plt.figure()\n","    fig, ax = plt.subplots(1)\n","    ax.imshow(img)\n","    # Rescale boxes to original image\n","    detections = rescale_boxes(detections, img_size, img.shape[:2])\n","    unique_labels = detections[:, -1].cpu().unique()\n","    n_cls_preds = len(unique_labels)\n","    # Bounding-box colors\n","    cmap = plt.get_cmap(\"tab20b\")\n","    colors = [cmap(i) for i in np.linspace(0, 1, n_cls_preds)]\n","    bbox_colors = random.sample(colors, n_cls_preds)\n","    # for x1, y1, x2, y2, conf, cls_pred in detections:\n","\n","    #     # if (conf >= 0.75): # Only plot boxes of 75% confidence or higher\n","    #     print(f\"\\t+ Label: {classes[int(cls_pred)]} | Confidence: {conf.item():0.4f}\")\n","\n","    #     box_w = x2 - x1\n","    #     box_h = y2 - y1\n","\n","    #     color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\n","    #     # Create a Rectangle patch\n","    #     bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2, edgecolor=color, facecolor=\"none\")\n","    #     # Add the bbox to the plot\n","    #     ax.add_patch(bbox)\n","    #     # Add label\n","    #     plt.text(\n","    #         x1,\n","    #         y1,\n","    #         s=classes[int(cls_pred)],\n","    #         color=\"white\",\n","    #         verticalalignment=\"top\",\n","    #         bbox={\"color\": color, \"pad\": 0})\n","\n","    x1, y1, x2, y2, conf, cls_pred = detections[0]\n","\n","    print(f\"\\t+ Label: {classes[int(cls_pred)]} | Confidence: {conf.item():0.4f}\")\n","\n","    box_w = x2 - x1\n","    box_h = y2 - y1\n","\n","    color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\n","    # Create a Rectangle patch\n","    bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2, edgecolor=color, facecolor=\"none\")\n","    # Add the bbox to the plot\n","    ax.add_patch(bbox)\n","    # Add label\n","    plt.text(\n","        x1,\n","        y1,\n","        s=classes[int(cls_pred)],\n","        color=\"white\",\n","        verticalalignment=\"top\",\n","        bbox={\"color\": color, \"pad\": 0})\n","\n","    # Save generated image with detections\n","    plt.axis(\"off\")\n","    plt.gca().xaxis.set_major_locator(NullLocator())\n","    plt.gca().yaxis.set_major_locator(NullLocator())\n","    filename = os.path.basename(image_path).split(\".\")[0]\n","    output_path = os.path.join(output_path, f\"{filename}.png\")\n","    #plt.savefig(output_path, bbox_inches=\"tight\", pad_inches=0.0)\n","    plt.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"su7vX7HEgoV5"},"source":["# Get dataloader\n","test_dataloader = torch.utils.data.DataLoader(\n","    ListDataset(test_path), batch_size=opt.batch_size, shuffle=False, num_workers=opt.n_cpu\n",")\n","\n","model.eval()\n","\n","test_results = []\n","\n","for batch_i, (test_paths, test_imgs, test_targets) in enumerate(test_dataloader):\n","  test_imgs = torch.tensor(test_imgs).cuda()\n","  \n","  with torch.no_grad():\n","    loss = model(test_imgs, test_targets)\n","    test_results.append([loss.item(), model.losses[\"recall\"], model.losses[\"precision\"]])\n","\n","final_results = np.average(test_results, axis=0)\n","print('Losses: total %f, recall: %.5f, precision: %.5f'\n","      %(final_results[0], final_results[1], final_results[2]))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rBwxt5BYzysj"},"source":["# Get dataloader\n","# test_dataloader = torch.utils.data.DataLoader(\n","#     ListDataset(test_path), batch_size=1, shuffle=False, num_workers=opt.n_cpu\n","# )\n","\n","# model.eval()\n","\n","# for batch_i, (test_paths, test_imgs, _) in enumerate(test_dataloader):\n","#   test_imgs = torch.tensor(test_imgs).cuda()\n","  \n","#   with torch.no_grad():\n","#     detections = model(test_imgs)\n","#     detections = non_max_suppression(detections)\n","\n","#   draw_and_save_output_image(test_paths[0], detections[0], test_imgs[0].shape, \"/content/drive/Shareddrives/COS-470/Gestures/data/gestures_dataset/output\", classes)"],"execution_count":null,"outputs":[]}]}